{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detector de anomalias en tarjetas de credito.\n",
    "\n",
    "### Contexto:\n",
    "Es importante que las compañías de tarjetas de crédito puedan reconocer transacciones fraudulentas de tarjetas de crédito para que a los clientes no se les cobre por los artículos que no compraron.\n",
    "\n",
    "### Contenido:\n",
    "Los conjuntos de datos contienen transacciones realizadas con tarjetas de crédito en septiembre de 2013 por titulares de tarjetas europeos. Este conjunto de datos presenta transacciones que ocurrieron en dos días, donde tenemos 492 fraudes de 284,807 transacciones. El conjunto de datos está altamente desequilibrado, la clase positiva (fraudes) representa el 0.172% de todas las transacciones.\n",
    "\n",
    "Contiene solo variables de entrada numéricas que son el resultado de una transformación PCA (Principal Component Analysis). Desafortunadamente, debido a problemas de confidencialidad, no podemos proporcionar las características originales y más información de fondo sobre los datos. Las características V1, V2, ... V28 son los componentes principales obtenidos con PCA, las únicas características que no se han transformado con PCA son 'Tiempo' y 'Cantidad'. La función 'Tiempo' contiene los segundos transcurridos entre cada transacción y la primera transacción en el conjunto de datos. La característica 'Cantidad' es la Cantidad de la transacción, esta característica se puede utilizar para el aprendizaje sensible al costo dependiente del ejemplo. La característica 'Clase' es la variable de respuesta y toma el valor 1 en caso de fraude y 0 en caso contrario.\n",
    "\n",
    "### Proposito:\n",
    "Identificar transacciones fraudulentas en tarjetas de credito\n",
    "\n",
    "Dada la relación de desequilibrio de clase, recomendamos medir la precisión utilizando el área bajo la curva de precisión-recuperación (AUPRC). La precisión de la matriz de confusión no es significativa para la clasificación desequilibrada.\n",
    "\n",
    "### Agradecimientos y creditos:\n",
    "El conjunto de datos se ha recopilado y analizado durante una colaboración de investigación de Worldline y el Machine Learning Group (http://mlg.ulb.ac.be) de ULB (Université Libre de Bruxelles) sobre minería de datos grandes y detección de fraude. Más detalles sobre proyectos actuales y pasados sobre temas relacionados están disponibles en https://www.researchgate.net/project/Fraud-detection-5 y la página del proyecto DefeatFraud y a Krish C Naik creador del repositorio de donde fue tomado este modelo disponible en GitHub https://github.com/krishnaik06/Credit-Card-Fraudlent usado bajo la licencia GNU General Public License v3.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 14, 8\n",
    "RANDOM_SEED = 42\n",
    "LABELS = [\"Normal\", \"Fraud\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('creditcard.csv',sep=',')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_classes = pd.value_counts(data['Class'], sort = True)\n",
    "\n",
    "count_classes.plot(kind = 'bar', rot=0)\n",
    "\n",
    "plt.title(\"Distribución de clase de transacción\")\n",
    "\n",
    "plt.xticks(range(2), LABELS)\n",
    "\n",
    "plt.xlabel(\"Class\")\n",
    "\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtenga el Fraude y el conjunto de datos normal \n",
    "\n",
    "fraud = data[data['Class']==1]\n",
    "\n",
    "normal = data[data['Class']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fraud.shape,normal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Necesitamos analizar más cantidad de información de los datos de la transacción.\n",
    "#¿Qué tan diferente es la cantidad de dinero utilizada en diferentes clases de transacciones? \n",
    "fraud.Amount.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal.Amount.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "f.suptitle('Amount per transaction by class')\n",
    "bins = 50\n",
    "ax1.hist(fraud.Amount, bins = bins)\n",
    "ax1.set_title('Fraud')\n",
    "ax2.hist(normal.Amount, bins = bins)\n",
    "ax2.set_title('Normal')\n",
    "plt.xlabel('Amount ($)')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.xlim((0, 20000))\n",
    "plt.yscale('log')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificaremos ¿Las transacciones fraudulentas ocurren con mayor frecuencia durante cierto período de tiempo? Descubrámoslo con una representación visual.\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "f.suptitle('Tiempo de transaccion vs Cantidad por clase')\n",
    "ax1.scatter(Fraud.Time, Fraud.Amount)\n",
    "ax1.set_title('Fraude')\n",
    "ax2.scatter(Normal.Time, Normal.Amount)\n",
    "ax2.set_title('Normal')\n",
    "plt.xlabel('Tiempo (en segundos)')\n",
    "plt.ylabel('Cantidad')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take some sample of the data\n",
    "\n",
    "data1= data.sample(frac = 0.1,random_state=1)\n",
    "\n",
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine el número de fraudes y transacciones válidas en el conjunto de datos \n",
    "\n",
    "Fraud = data1[data1['Class']==1]\n",
    "\n",
    "Valid = data1[data1['Class']==0]\n",
    "\n",
    "outlier_fraction = len(Fraud)/float(len(Valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outlier_fraction)\n",
    "\n",
    "print(\"Casos de fraude: {}\".format(len(Fraud)))\n",
    "\n",
    "print(\"Casos validos: {}\".format(len(Valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correlation\n",
    "import seaborn as sns\n",
    "#get correlations of each features in dataset\n",
    "corrmat = data1.corr()\n",
    "top_corr_features = corrmat.index\n",
    "plt.figure(figsize=(20,20))\n",
    "#plot heat map\n",
    "g=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear características independientes y dependientes \n",
    "columns = data1.columns.tolist()\n",
    "# Filtre las columnas para eliminar datos que no queremos \n",
    "columns = [c for c in columns if c not in [\"Class\"]]\n",
    "# Almacene la variable que estamos prediciendo \n",
    "target = \"Class\"\n",
    "# Definir un estado aleatorio. \n",
    "state = np.random.RandomState(42)\n",
    "X = data1[columns]\n",
    "Y = data1[target]\n",
    "X_outliers = state.uniform(low=0, high=1, size=(X.shape[0], X.shape[1]))\n",
    "# Print the shapes of X & Y\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicción del modelo\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ahora es el momento de comenzar a construir el modelo. Los tipos de algoritmos que vamos a utilizar para tratar de detectar anomalías en este conjunto de datos son los siguientes\n",
    "\n",
    "### Aislamiento del Algoritmo Forestal :\n",
    "Una de las técnicas más nuevas para detectar anomalías se llama bosques de aislamiento. El algoritmo se basa en el hecho de que las anomalías son puntos de datos que son pocos y diferentes. Como resultado de estas propiedades, las anomalías son susceptibles a un mecanismo llamado aislamiento.\n",
    "\n",
    "Este método es muy útil y es fundamentalmente diferente de todos los métodos existentes. Introduce el uso del aislamiento como un medio más efectivo y eficiente para detectar anomalías que las medidas básicas de distancia y densidad comúnmente utilizadas. Además, este método es un algoritmo con una baja complejidad de tiempo lineal y un pequeño requisito de memoria. Construye un modelo de buen rendimiento con una pequeña cantidad de árboles que utilizan pequeñas submuestras de tamaño fijo, independientemente del tamaño de un conjunto de datos. \n",
    "\n",
    "Los métodos típicos de aprendizaje automático tienden a funcionar mejor cuando los patrones que intentan aprender son equilibrados, lo que significa que la misma cantidad de comportamientos buenos y malos están presentes en el conjunto de datos.\n",
    "\n",
    "Como funciona el bosque de aislamiento?\n",
    "El algoritmo de bosque de aislamiento aísla las observaciones seleccionando aleatoriamente una característica y luego seleccionando aleatoriamente un valor dividido entre los valores máximo y mínimo de la característica seleccionada. El argumento lógico dice: aislar las observaciones de anomalías es más fácil porque solo se necesitan unas pocas condiciones para separar esos casos de las observaciones normales. Por otro lado, aislar observaciones normales requiere más condiciones. Por lo tanto, se puede calcular un puntaje de anomalía como el número de condiciones requeridas para separar una observación dada.\n",
    "\n",
    "La forma en que el algoritmo construye la separación es primero creando árboles de aislamiento o árboles de decisión aleatorios. Luego, la puntuación se calcula como la longitud de la ruta para aislar la observación.\n",
    "\n",
    "\n",
    "### Algoritmo de factor de valor atípico local (LOF)\n",
    "El algoritmo LOF es un método de detección de valores atípicos no supervisados que calcula la desviación de densidad local de un punto de datos dado con respecto a sus vecinos. Considera como muestras atípicas que tienen una densidad sustancialmente menor que sus vecinas.\n",
    "\n",
    "El número de vecinos considerado (parámetro n_neighbours) generalmente se elige 1) mayor que el número mínimo de objetos que debe contener un clúster, para que otros objetos puedan ser valores atípicos locales en relación con este clúster y 2) más pequeño que el número máximo de cerca de objetos que potencialmente pueden ser valores atípicos locales. En la práctica, tales informaciones generalmente no están disponibles, y tomar n_neighbours = 20 parece funcionar bien en general.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Definir los métodos de detección de valores atípicos.\n",
    "\n",
    "classifiers = {\n",
    "    \"Isolation Forest\":IsolationForest(n_estimators=100, max_samples=len(X), \n",
    "                                       contamination=outlier_fraction,random_state=state, verbose=0),\n",
    "    \"Local Outlier Factor\":LocalOutlierFactor(n_neighbors=20, algorithm='auto', \n",
    "                                              leaf_size=30, metric='minkowski',\n",
    "                                              p=2, metric_params=None, contamination=outlier_fraction),\n",
    "    \"Support Vector Machine\":OneClassSVM(kernel='rbf', degree=3, gamma=0.1,nu=0.05, \n",
    "                                         max_iter=-1, random_state=state)\n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_outliers = len(Fraud)\n",
    "for i, (clf_name,clf) in enumerate(classifiers.items()):\n",
    "    #Ajustar los datos y etiquetar valores atípicos\n",
    "    if clf_name == \"Local Outlier Factor\":\n",
    "        y_pred = clf.fit_predict(X)\n",
    "        scores_prediction = clf.negative_outlier_factor_\n",
    "    elif clf_name == \"Support Vector Machine\":\n",
    "        clf.fit(X)\n",
    "        y_pred = clf.predict(X)\n",
    "    else:    \n",
    "        clf.fit(X)\n",
    "        scores_prediction = clf.decision_function(X)\n",
    "        y_pred = clf.predict(X)\n",
    "#Cambie la forma de los valores de predicción a 0 para transacciones válidas, 1 para transacciones de fraude    y_pred[y_pred == 1] = 0\n",
    "    y_pred[y_pred == -1] = 1\n",
    "    n_errors = (y_pred != Y).sum()\n",
    "# Ejecutar métricas de clasificación\n",
    "    print(\"{}: {}\".format(clf_name,n_errors))\n",
    "    print(\"Accuracy Score :\")\n",
    "    print(accuracy_score(Y,y_pred))\n",
    "    print(\"Classification Report :\")\n",
    "    print(classification_report(Y,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observaciones:\n",
    "- El bosque de aislamiento detectó 73 errores frente al factor de valor atípico local que detectó 97 errores frente a SVM que detectó 8516 errores\n",
    "- El bosque de aislamiento tiene un 99.74% más de precisión que un LOF de 99.65% y un SVM de 70.09\n",
    "- Al comparar la precisión y recuperación de errores para 3 modelos, el Bosque de aislamiento funcionó mucho mejor que el LOF, ya que podemos ver que la detección de casos de fraude es de alrededor del 27% frente a la tasa de detección de LOF de solo el 2% y SVM del 0%.\n",
    "- Por lo tanto, el método general de bosque de aislamiento funcionó mucho mejor para determinar los casos de fraude, que es de alrededor del 30%.\n",
    "- También podemos mejorar esta precisión al aumentar el tamaño de la muestra o usar algoritmos de aprendizaje profundo, sin embargo, a costa del gasto computacional. También podemos usar modelos de detección de anomalías complejas para obtener una mayor precisión en la determinación de casos más fraudulentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 32-bit",
   "language": "python",
   "name": "python37332bit3400474cd4ce47b78f15c406da8fff3e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}